{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Майнор \"Интеллектуальный анализ данных\" </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Курс \"Современные методы машинного обучения\" </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Лабораторная работа №1. Image Classification. </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной лабораторной работе вам предлагается обучить модель на основе нейронной сети для распознавания рукописных букв английского алфавита."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные представлены двумя датасетами: обучающим (`train`) и тестовым (`test`). Изображения для каждого датасета находятся в `images.zip`.  \n",
    "  \n",
    "Обучающая выборка состоит из 65000 изображений - по 2500 изображений для каждой буквы.  \n",
    "Тестовая выборка состоит из 13000 изображений - по 500 изображений для каждой буквы.  \n",
    "  \n",
    "Все изображения - монохромные (но в формате RGB), размерности $28 \\times 28$ пикселей, в формате JPEG. \n",
    "В названии каждого файла содержатся буква, которая представлена на изображении, и уникальный номер изображения: `a_00002.jpg`.  \n",
    "  \n",
    "**NB:** Все изображения представлены в перевернутом виде, для корректного отображения их нужно сначала транспонировать. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 3)\n"
     ]
    }
   ],
   "source": [
    "pic = plt.imread('images/train/a/a_00002.jpg')\n",
    "print(pic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic = np.transpose(pic, axes=(1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(pic)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ваша задача - создать и обучить модель на основе нейронной сети, которая будет предсказывать букву на картинке.  \n",
    "Обучение необходимо проводить на данных из `train`, качество модели проверять на данных из `test`.  \n",
    "Целевая метрика - accuracy.  \n",
    "Для моделирования необходимо использовать `pytorch`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основные задания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_and_targets_lst(path):\n",
    "    data=[]\n",
    "    targets=[]\n",
    "    for root, currentDirectory, files in os.walk(path):\n",
    "        for file in files:\n",
    "            pic = plt.imread(os.path.join(root, file))\n",
    "            pic = torch.tensor(np.transpose(pic, axes=(1, 0, 2)))\n",
    "            pic = torch.div(pic, 255)\n",
    "            target = ord(file[0]) - ord('a')\n",
    "            data.append(pic)\n",
    "            targets.append(target)\n",
    "    return data,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset): \n",
    "    \n",
    "    def __init__(self, x, y):\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #return {'sample': self.x[idx], 'target': self.y[idx]}\n",
    "        return self.x[idx],self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = get_data_and_targets_lst(\"images/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[60000].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[60000])\n",
    "print(y[60000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13000, 13000)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(X_train, y_train)\n",
    "test_dataset = MyDataset(X_test, y_test)\n",
    "#train_dataset.__getitem__(40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([11,  3, 11, 16, 12,  1, 21, 14,  0,  6,  3, 15, 16, 12,  4, 25, 24, 24,\n",
      "        20, 13,  6, 21, 15, 15, 22, 15, 10,  7, 11,  4, 13,  4])\n",
      "Shape of X [N, C, H, W]:  torch.Size([32, 28, 28, 3])\n",
      "Shape of y:  torch.Size([32]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size=32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "for batch, (X, y) in enumerate(train_dataloader):\n",
    "    print(batch)\n",
    "    print(y)\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "    #NHWC now\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_cycle(epochs, model, loss_fn, optimizer):\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(train_dataloader, model, loss_fn, optimizer)\n",
    "        test(test_dataloader, model, loss_fn)\n",
    "        #print(\"Done!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28*3, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 26),\n",
    "            nn.Softmax(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lin3 = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_lin3.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.257846  [    0/52000]\n",
      "loss: 3.258039  [ 3200/52000]\n",
      "loss: 3.258527  [ 6400/52000]\n",
      "loss: 3.257884  [ 9600/52000]\n",
      "loss: 3.258196  [12800/52000]\n",
      "loss: 3.258030  [16000/52000]\n",
      "loss: 3.258174  [19200/52000]\n",
      "loss: 3.257998  [22400/52000]\n",
      "loss: 3.257539  [25600/52000]\n",
      "loss: 3.257842  [28800/52000]\n",
      "loss: 3.257426  [32000/52000]\n",
      "loss: 3.257658  [35200/52000]\n",
      "loss: 3.257893  [38400/52000]\n",
      "loss: 3.257926  [41600/52000]\n",
      "loss: 3.258086  [44800/52000]\n",
      "loss: 3.258446  [48000/52000]\n",
      "loss: 3.258027  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 4.6%, Avg loss: 3.257965 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.257738  [    0/52000]\n",
      "loss: 3.257934  [ 3200/52000]\n",
      "loss: 3.258486  [ 6400/52000]\n",
      "loss: 3.257786  [ 9600/52000]\n",
      "loss: 3.258088  [12800/52000]\n",
      "loss: 3.257925  [16000/52000]\n",
      "loss: 3.258070  [19200/52000]\n",
      "loss: 3.257930  [22400/52000]\n",
      "loss: 3.257439  [25600/52000]\n",
      "loss: 3.257738  [28800/52000]\n",
      "loss: 3.257317  [32000/52000]\n",
      "loss: 3.257595  [35200/52000]\n",
      "loss: 3.257794  [38400/52000]\n",
      "loss: 3.257800  [41600/52000]\n",
      "loss: 3.257976  [44800/52000]\n",
      "loss: 3.258317  [48000/52000]\n",
      "loss: 3.257899  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 5.2%, Avg loss: 3.257869 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.257629  [    0/52000]\n",
      "loss: 3.257826  [ 3200/52000]\n",
      "loss: 3.258446  [ 6400/52000]\n",
      "loss: 3.257688  [ 9600/52000]\n",
      "loss: 3.257977  [12800/52000]\n",
      "loss: 3.257820  [16000/52000]\n",
      "loss: 3.257965  [19200/52000]\n",
      "loss: 3.257861  [22400/52000]\n",
      "loss: 3.257340  [25600/52000]\n",
      "loss: 3.257633  [28800/52000]\n",
      "loss: 3.257208  [32000/52000]\n",
      "loss: 3.257532  [35200/52000]\n",
      "loss: 3.257694  [38400/52000]\n",
      "loss: 3.257673  [41600/52000]\n",
      "loss: 3.257866  [44800/52000]\n",
      "loss: 3.258186  [48000/52000]\n",
      "loss: 3.257769  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 5.9%, Avg loss: 3.257772 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.257519  [    0/52000]\n",
      "loss: 3.257717  [ 3200/52000]\n",
      "loss: 3.258406  [ 6400/52000]\n",
      "loss: 3.257590  [ 9600/52000]\n",
      "loss: 3.257864  [12800/52000]\n",
      "loss: 3.257716  [16000/52000]\n",
      "loss: 3.257858  [19200/52000]\n",
      "loss: 3.257792  [22400/52000]\n",
      "loss: 3.257238  [25600/52000]\n",
      "loss: 3.257526  [28800/52000]\n",
      "loss: 3.257097  [32000/52000]\n",
      "loss: 3.257466  [35200/52000]\n",
      "loss: 3.257594  [38400/52000]\n",
      "loss: 3.257542  [41600/52000]\n",
      "loss: 3.257755  [44800/52000]\n",
      "loss: 3.258051  [48000/52000]\n",
      "loss: 3.257636  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 6.7%, Avg loss: 3.257675 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.257406  [    0/52000]\n",
      "loss: 3.257608  [ 3200/52000]\n",
      "loss: 3.258366  [ 6400/52000]\n",
      "loss: 3.257489  [ 9600/52000]\n",
      "loss: 3.257748  [12800/52000]\n",
      "loss: 3.257610  [16000/52000]\n",
      "loss: 3.257749  [19200/52000]\n",
      "loss: 3.257721  [22400/52000]\n",
      "loss: 3.257137  [25600/52000]\n",
      "loss: 3.257418  [28800/52000]\n",
      "loss: 3.256981  [32000/52000]\n",
      "loss: 3.257399  [35200/52000]\n",
      "loss: 3.257492  [38400/52000]\n",
      "loss: 3.257410  [41600/52000]\n",
      "loss: 3.257641  [44800/52000]\n",
      "loss: 3.257913  [48000/52000]\n",
      "loss: 3.257501  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 7.5%, Avg loss: 3.257575 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 3.257291  [    0/52000]\n",
      "loss: 3.257496  [ 3200/52000]\n",
      "loss: 3.258325  [ 6400/52000]\n",
      "loss: 3.257386  [ 9600/52000]\n",
      "loss: 3.257630  [12800/52000]\n",
      "loss: 3.257502  [16000/52000]\n",
      "loss: 3.257639  [19200/52000]\n",
      "loss: 3.257646  [22400/52000]\n",
      "loss: 3.257031  [25600/52000]\n",
      "loss: 3.257310  [28800/52000]\n",
      "loss: 3.256865  [32000/52000]\n",
      "loss: 3.257332  [35200/52000]\n",
      "loss: 3.257389  [38400/52000]\n",
      "loss: 3.257275  [41600/52000]\n",
      "loss: 3.257522  [44800/52000]\n",
      "loss: 3.257774  [48000/52000]\n",
      "loss: 3.257360  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 8.4%, Avg loss: 3.257474 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 3.257175  [    0/52000]\n",
      "loss: 3.257381  [ 3200/52000]\n",
      "loss: 3.258283  [ 6400/52000]\n",
      "loss: 3.257282  [ 9600/52000]\n",
      "loss: 3.257509  [12800/52000]\n",
      "loss: 3.257394  [16000/52000]\n",
      "loss: 3.257527  [19200/52000]\n",
      "loss: 3.257568  [22400/52000]\n",
      "loss: 3.256926  [25600/52000]\n",
      "loss: 3.257200  [28800/52000]\n",
      "loss: 3.256746  [32000/52000]\n",
      "loss: 3.257263  [35200/52000]\n",
      "loss: 3.257285  [38400/52000]\n",
      "loss: 3.257136  [41600/52000]\n",
      "loss: 3.257401  [44800/52000]\n",
      "loss: 3.257632  [48000/52000]\n",
      "loss: 3.257217  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 9.3%, Avg loss: 3.257371 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 3.257059  [    0/52000]\n",
      "loss: 3.257262  [ 3200/52000]\n",
      "loss: 3.258241  [ 6400/52000]\n",
      "loss: 3.257179  [ 9600/52000]\n",
      "loss: 3.257387  [12800/52000]\n",
      "loss: 3.257283  [16000/52000]\n",
      "loss: 3.257412  [19200/52000]\n",
      "loss: 3.257487  [22400/52000]\n",
      "loss: 3.256816  [25600/52000]\n",
      "loss: 3.257086  [28800/52000]\n",
      "loss: 3.256623  [32000/52000]\n",
      "loss: 3.257193  [35200/52000]\n",
      "loss: 3.257179  [38400/52000]\n",
      "loss: 3.256993  [41600/52000]\n",
      "loss: 3.257279  [44800/52000]\n",
      "loss: 3.257486  [48000/52000]\n",
      "loss: 3.257071  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 10.1%, Avg loss: 3.257265 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 3.256938  [    0/52000]\n",
      "loss: 3.257139  [ 3200/52000]\n",
      "loss: 3.258197  [ 6400/52000]\n",
      "loss: 3.257072  [ 9600/52000]\n",
      "loss: 3.257263  [12800/52000]\n",
      "loss: 3.257171  [16000/52000]\n",
      "loss: 3.257295  [19200/52000]\n",
      "loss: 3.257404  [22400/52000]\n",
      "loss: 3.256703  [25600/52000]\n",
      "loss: 3.256971  [28800/52000]\n",
      "loss: 3.256496  [32000/52000]\n",
      "loss: 3.257122  [35200/52000]\n",
      "loss: 3.257070  [38400/52000]\n",
      "loss: 3.256845  [41600/52000]\n",
      "loss: 3.257154  [44800/52000]\n",
      "loss: 3.257336  [48000/52000]\n",
      "loss: 3.256920  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 10.9%, Avg loss: 3.257157 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 3.256814  [    0/52000]\n",
      "loss: 3.257009  [ 3200/52000]\n",
      "loss: 3.258152  [ 6400/52000]\n",
      "loss: 3.256962  [ 9600/52000]\n",
      "loss: 3.257135  [12800/52000]\n",
      "loss: 3.257056  [16000/52000]\n",
      "loss: 3.257175  [19200/52000]\n",
      "loss: 3.257319  [22400/52000]\n",
      "loss: 3.256585  [25600/52000]\n",
      "loss: 3.256852  [28800/52000]\n",
      "loss: 3.256365  [32000/52000]\n",
      "loss: 3.257048  [35200/52000]\n",
      "loss: 3.256958  [38400/52000]\n",
      "loss: 3.256693  [41600/52000]\n",
      "loss: 3.257026  [44800/52000]\n",
      "loss: 3.257181  [48000/52000]\n",
      "loss: 3.256763  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 11.6%, Avg loss: 3.257046 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 3.256685  [    0/52000]\n",
      "loss: 3.256873  [ 3200/52000]\n",
      "loss: 3.258108  [ 6400/52000]\n",
      "loss: 3.256850  [ 9600/52000]\n",
      "loss: 3.257004  [12800/52000]\n",
      "loss: 3.256938  [16000/52000]\n",
      "loss: 3.257052  [19200/52000]\n",
      "loss: 3.257231  [22400/52000]\n",
      "loss: 3.256462  [25600/52000]\n",
      "loss: 3.256730  [28800/52000]\n",
      "loss: 3.256231  [32000/52000]\n",
      "loss: 3.256971  [35200/52000]\n",
      "loss: 3.256843  [38400/52000]\n",
      "loss: 3.256535  [41600/52000]\n",
      "loss: 3.256895  [44800/52000]\n",
      "loss: 3.257020  [48000/52000]\n",
      "loss: 3.256599  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 12.4%, Avg loss: 3.256931 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 3.256550  [    0/52000]\n",
      "loss: 3.256732  [ 3200/52000]\n",
      "loss: 3.258063  [ 6400/52000]\n",
      "loss: 3.256735  [ 9600/52000]\n",
      "loss: 3.256869  [12800/52000]\n",
      "loss: 3.256818  [16000/52000]\n",
      "loss: 3.256924  [19200/52000]\n",
      "loss: 3.257139  [22400/52000]\n",
      "loss: 3.256336  [25600/52000]\n",
      "loss: 3.256607  [28800/52000]\n",
      "loss: 3.256092  [32000/52000]\n",
      "loss: 3.256893  [35200/52000]\n",
      "loss: 3.256726  [38400/52000]\n",
      "loss: 3.256370  [41600/52000]\n",
      "loss: 3.256760  [44800/52000]\n",
      "loss: 3.256852  [48000/52000]\n",
      "loss: 3.256427  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 13.2%, Avg loss: 3.256812 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 3.256412  [    0/52000]\n",
      "loss: 3.256584  [ 3200/52000]\n",
      "loss: 3.258018  [ 6400/52000]\n",
      "loss: 3.256616  [ 9600/52000]\n",
      "loss: 3.256729  [12800/52000]\n",
      "loss: 3.256695  [16000/52000]\n",
      "loss: 3.256791  [19200/52000]\n",
      "loss: 3.257045  [22400/52000]\n",
      "loss: 3.256206  [25600/52000]\n",
      "loss: 3.256478  [28800/52000]\n",
      "loss: 3.255947  [32000/52000]\n",
      "loss: 3.256811  [35200/52000]\n",
      "loss: 3.256603  [38400/52000]\n",
      "loss: 3.256199  [41600/52000]\n",
      "loss: 3.256622  [44800/52000]\n",
      "loss: 3.256679  [48000/52000]\n",
      "loss: 3.256247  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 14.1%, Avg loss: 3.256689 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 3.256268  [    0/52000]\n",
      "loss: 3.256430  [ 3200/52000]\n",
      "loss: 3.257972  [ 6400/52000]\n",
      "loss: 3.256494  [ 9600/52000]\n",
      "loss: 3.256585  [12800/52000]\n",
      "loss: 3.256567  [16000/52000]\n",
      "loss: 3.256650  [19200/52000]\n",
      "loss: 3.256947  [22400/52000]\n",
      "loss: 3.256073  [25600/52000]\n",
      "loss: 3.256347  [28800/52000]\n",
      "loss: 3.255797  [32000/52000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.256726  [35200/52000]\n",
      "loss: 3.256476  [38400/52000]\n",
      "loss: 3.256023  [41600/52000]\n",
      "loss: 3.256479  [44800/52000]\n",
      "loss: 3.256498  [48000/52000]\n",
      "loss: 3.256058  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 14.9%, Avg loss: 3.256561 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 3.256121  [    0/52000]\n",
      "loss: 3.256269  [ 3200/52000]\n",
      "loss: 3.257924  [ 6400/52000]\n",
      "loss: 3.256368  [ 9600/52000]\n",
      "loss: 3.256438  [12800/52000]\n",
      "loss: 3.256433  [16000/52000]\n",
      "loss: 3.256505  [19200/52000]\n",
      "loss: 3.256847  [22400/52000]\n",
      "loss: 3.255935  [25600/52000]\n",
      "loss: 3.256211  [28800/52000]\n",
      "loss: 3.255640  [32000/52000]\n",
      "loss: 3.256636  [35200/52000]\n",
      "loss: 3.256342  [38400/52000]\n",
      "loss: 3.255836  [41600/52000]\n",
      "loss: 3.256331  [44800/52000]\n",
      "loss: 3.256308  [48000/52000]\n",
      "loss: 3.255859  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 16.0%, Avg loss: 3.256428 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 3.255969  [    0/52000]\n",
      "loss: 3.256098  [ 3200/52000]\n",
      "loss: 3.257874  [ 6400/52000]\n",
      "loss: 3.256237  [ 9600/52000]\n",
      "loss: 3.256284  [12800/52000]\n",
      "loss: 3.256292  [16000/52000]\n",
      "loss: 3.256355  [19200/52000]\n",
      "loss: 3.256742  [22400/52000]\n",
      "loss: 3.255790  [25600/52000]\n",
      "loss: 3.256071  [28800/52000]\n",
      "loss: 3.255477  [32000/52000]\n",
      "loss: 3.256545  [35200/52000]\n",
      "loss: 3.256203  [38400/52000]\n",
      "loss: 3.255641  [41600/52000]\n",
      "loss: 3.256179  [44800/52000]\n",
      "loss: 3.256109  [48000/52000]\n",
      "loss: 3.255650  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 17.2%, Avg loss: 3.256289 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 3.255811  [    0/52000]\n",
      "loss: 3.255917  [ 3200/52000]\n",
      "loss: 3.257822  [ 6400/52000]\n",
      "loss: 3.256100  [ 9600/52000]\n",
      "loss: 3.256121  [12800/52000]\n",
      "loss: 3.256143  [16000/52000]\n",
      "loss: 3.256196  [19200/52000]\n",
      "loss: 3.256632  [22400/52000]\n",
      "loss: 3.255638  [25600/52000]\n",
      "loss: 3.255924  [28800/52000]\n",
      "loss: 3.255305  [32000/52000]\n",
      "loss: 3.256450  [35200/52000]\n",
      "loss: 3.256058  [38400/52000]\n",
      "loss: 3.255437  [41600/52000]\n",
      "loss: 3.256022  [44800/52000]\n",
      "loss: 3.255899  [48000/52000]\n",
      "loss: 3.255429  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 18.3%, Avg loss: 3.256144 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 3.255647  [    0/52000]\n",
      "loss: 3.255725  [ 3200/52000]\n",
      "loss: 3.257769  [ 6400/52000]\n",
      "loss: 3.255957  [ 9600/52000]\n",
      "loss: 3.255948  [12800/52000]\n",
      "loss: 3.255986  [16000/52000]\n",
      "loss: 3.256027  [19200/52000]\n",
      "loss: 3.256517  [22400/52000]\n",
      "loss: 3.255482  [25600/52000]\n",
      "loss: 3.255772  [28800/52000]\n",
      "loss: 3.255122  [32000/52000]\n",
      "loss: 3.256348  [35200/52000]\n",
      "loss: 3.255906  [38400/52000]\n",
      "loss: 3.255224  [41600/52000]\n",
      "loss: 3.255857  [44800/52000]\n",
      "loss: 3.255677  [48000/52000]\n",
      "loss: 3.255193  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 19.5%, Avg loss: 3.255992 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 3.255476  [    0/52000]\n",
      "loss: 3.255524  [ 3200/52000]\n",
      "loss: 3.257714  [ 6400/52000]\n",
      "loss: 3.255807  [ 9600/52000]\n",
      "loss: 3.255766  [12800/52000]\n",
      "loss: 3.255823  [16000/52000]\n",
      "loss: 3.255849  [19200/52000]\n",
      "loss: 3.256394  [22400/52000]\n",
      "loss: 3.255320  [25600/52000]\n",
      "loss: 3.255611  [28800/52000]\n",
      "loss: 3.254930  [32000/52000]\n",
      "loss: 3.256241  [35200/52000]\n",
      "loss: 3.255747  [38400/52000]\n",
      "loss: 3.254998  [41600/52000]\n",
      "loss: 3.255684  [44800/52000]\n",
      "loss: 3.255446  [48000/52000]\n",
      "loss: 3.254941  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 20.6%, Avg loss: 3.255833 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 3.255296  [    0/52000]\n",
      "loss: 3.255314  [ 3200/52000]\n",
      "loss: 3.257656  [ 6400/52000]\n",
      "loss: 3.255650  [ 9600/52000]\n",
      "loss: 3.255574  [12800/52000]\n",
      "loss: 3.255652  [16000/52000]\n",
      "loss: 3.255663  [19200/52000]\n",
      "loss: 3.256267  [22400/52000]\n",
      "loss: 3.255150  [25600/52000]\n",
      "loss: 3.255445  [28800/52000]\n",
      "loss: 3.254729  [32000/52000]\n",
      "loss: 3.256128  [35200/52000]\n",
      "loss: 3.255579  [38400/52000]\n",
      "loss: 3.254760  [41600/52000]\n",
      "loss: 3.255503  [44800/52000]\n",
      "loss: 3.255203  [48000/52000]\n",
      "loss: 3.254669  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 21.3%, Avg loss: 3.255665 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 3.255110  [    0/52000]\n",
      "loss: 3.255091  [ 3200/52000]\n",
      "loss: 3.257598  [ 6400/52000]\n",
      "loss: 3.255485  [ 9600/52000]\n",
      "loss: 3.255373  [12800/52000]\n",
      "loss: 3.255473  [16000/52000]\n",
      "loss: 3.255464  [19200/52000]\n",
      "loss: 3.256133  [22400/52000]\n",
      "loss: 3.254971  [25600/52000]\n",
      "loss: 3.255272  [28800/52000]\n",
      "loss: 3.254519  [32000/52000]\n",
      "loss: 3.256012  [35200/52000]\n",
      "loss: 3.255403  [38400/52000]\n",
      "loss: 3.254508  [41600/52000]\n",
      "loss: 3.255313  [44800/52000]\n",
      "loss: 3.254945  [48000/52000]\n",
      "loss: 3.254379  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 21.8%, Avg loss: 3.255488 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 3.254915  [    0/52000]\n",
      "loss: 3.254854  [ 3200/52000]\n",
      "loss: 3.257539  [ 6400/52000]\n",
      "loss: 3.255312  [ 9600/52000]\n",
      "loss: 3.255157  [12800/52000]\n",
      "loss: 3.255283  [16000/52000]\n",
      "loss: 3.255252  [19200/52000]\n",
      "loss: 3.255992  [22400/52000]\n",
      "loss: 3.254784  [25600/52000]\n",
      "loss: 3.255089  [28800/52000]\n",
      "loss: 3.254298  [32000/52000]\n",
      "loss: 3.255889  [35200/52000]\n",
      "loss: 3.255215  [38400/52000]\n",
      "loss: 3.254241  [41600/52000]\n",
      "loss: 3.255114  [44800/52000]\n",
      "loss: 3.254669  [48000/52000]\n",
      "loss: 3.254064  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 22.2%, Avg loss: 3.255300 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 3.254709  [    0/52000]\n",
      "loss: 3.254601  [ 3200/52000]\n",
      "loss: 3.257478  [ 6400/52000]\n",
      "loss: 3.255132  [ 9600/52000]\n",
      "loss: 3.254930  [12800/52000]\n",
      "loss: 3.255080  [16000/52000]\n",
      "loss: 3.255023  [19200/52000]\n",
      "loss: 3.255842  [22400/52000]\n",
      "loss: 3.254589  [25600/52000]\n",
      "loss: 3.254896  [28800/52000]\n",
      "loss: 3.254064  [32000/52000]\n",
      "loss: 3.255759  [35200/52000]\n",
      "loss: 3.255018  [38400/52000]\n",
      "loss: 3.253954  [41600/52000]\n",
      "loss: 3.254903  [44800/52000]\n",
      "loss: 3.254375  [48000/52000]\n",
      "loss: 3.253725  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 22.6%, Avg loss: 3.255101 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 3.254489  [    0/52000]\n",
      "loss: 3.254327  [ 3200/52000]\n",
      "loss: 3.257416  [ 6400/52000]\n",
      "loss: 3.254942  [ 9600/52000]\n",
      "loss: 3.254688  [12800/52000]\n",
      "loss: 3.254861  [16000/52000]\n",
      "loss: 3.254772  [19200/52000]\n",
      "loss: 3.255683  [22400/52000]\n",
      "loss: 3.254385  [25600/52000]\n",
      "loss: 3.254691  [28800/52000]\n",
      "loss: 3.253818  [32000/52000]\n",
      "loss: 3.255622  [35200/52000]\n",
      "loss: 3.254808  [38400/52000]\n",
      "loss: 3.253648  [41600/52000]\n",
      "loss: 3.254681  [44800/52000]\n",
      "loss: 3.254063  [48000/52000]\n",
      "loss: 3.253356  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 22.9%, Avg loss: 3.254890 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 3.254258  [    0/52000]\n",
      "loss: 3.254033  [ 3200/52000]\n",
      "loss: 3.257350  [ 6400/52000]\n",
      "loss: 3.254742  [ 9600/52000]\n",
      "loss: 3.254428  [12800/52000]\n",
      "loss: 3.254627  [16000/52000]\n",
      "loss: 3.254495  [19200/52000]\n",
      "loss: 3.255514  [22400/52000]\n",
      "loss: 3.254171  [25600/52000]\n",
      "loss: 3.254475  [28800/52000]\n",
      "loss: 3.253558  [32000/52000]\n",
      "loss: 3.255474  [35200/52000]\n",
      "loss: 3.254586  [38400/52000]\n",
      "loss: 3.253320  [41600/52000]\n",
      "loss: 3.254442  [44800/52000]\n",
      "loss: 3.253733  [48000/52000]\n",
      "loss: 3.252952  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 22.8%, Avg loss: 3.254664 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 3.254011  [    0/52000]\n",
      "loss: 3.253715  [ 3200/52000]\n",
      "loss: 3.257281  [ 6400/52000]\n",
      "loss: 3.254530  [ 9600/52000]\n",
      "loss: 3.254148  [12800/52000]\n",
      "loss: 3.254376  [16000/52000]\n",
      "loss: 3.254189  [19200/52000]\n",
      "loss: 3.255333  [22400/52000]\n",
      "loss: 3.253946  [25600/52000]\n",
      "loss: 3.254244  [28800/52000]\n",
      "loss: 3.253280  [32000/52000]\n",
      "loss: 3.255317  [35200/52000]\n",
      "loss: 3.254349  [38400/52000]\n",
      "loss: 3.252967  [41600/52000]\n",
      "loss: 3.254187  [44800/52000]\n",
      "loss: 3.253379  [48000/52000]\n",
      "loss: 3.252507  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 22.8%, Avg loss: 3.254422 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 3.253748  [    0/52000]\n",
      "loss: 3.253369  [ 3200/52000]\n",
      "loss: 3.257208  [ 6400/52000]\n",
      "loss: 3.254304  [ 9600/52000]\n",
      "loss: 3.253849  [12800/52000]\n",
      "loss: 3.254103  [16000/52000]\n",
      "loss: 3.253851  [19200/52000]\n",
      "loss: 3.255138  [22400/52000]\n",
      "loss: 3.253708  [25600/52000]\n",
      "loss: 3.253996  [28800/52000]\n",
      "loss: 3.252987  [32000/52000]\n",
      "loss: 3.255149  [35200/52000]\n",
      "loss: 3.254099  [38400/52000]\n",
      "loss: 3.252584  [41600/52000]\n",
      "loss: 3.253916  [44800/52000]\n",
      "loss: 3.252995  [48000/52000]\n",
      "loss: 3.252009  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 22.1%, Avg loss: 3.254162 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 3.253469  [    0/52000]\n",
      "loss: 3.252989  [ 3200/52000]\n",
      "loss: 3.257135  [ 6400/52000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.254063  [ 9600/52000]\n",
      "loss: 3.253520  [12800/52000]\n",
      "loss: 3.253806  [16000/52000]\n",
      "loss: 3.253472  [19200/52000]\n",
      "loss: 3.254928  [22400/52000]\n",
      "loss: 3.253455  [25600/52000]\n",
      "loss: 3.253730  [28800/52000]\n",
      "loss: 3.252678  [32000/52000]\n",
      "loss: 3.254968  [35200/52000]\n",
      "loss: 3.253834  [38400/52000]\n",
      "loss: 3.252163  [41600/52000]\n",
      "loss: 3.253624  [44800/52000]\n",
      "loss: 3.252578  [48000/52000]\n",
      "loss: 3.251450  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 21.4%, Avg loss: 3.253881 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 3.253172  [    0/52000]\n",
      "loss: 3.252567  [ 3200/52000]\n",
      "loss: 3.257057  [ 6400/52000]\n",
      "loss: 3.253804  [ 9600/52000]\n",
      "loss: 3.253159  [12800/52000]\n",
      "loss: 3.253482  [16000/52000]\n",
      "loss: 3.253047  [19200/52000]\n",
      "loss: 3.254700  [22400/52000]\n",
      "loss: 3.253187  [25600/52000]\n",
      "loss: 3.253441  [28800/52000]\n",
      "loss: 3.252352  [32000/52000]\n",
      "loss: 3.254776  [35200/52000]\n",
      "loss: 3.253553  [38400/52000]\n",
      "loss: 3.251703  [41600/52000]\n",
      "loss: 3.253312  [44800/52000]\n",
      "loss: 3.252120  [48000/52000]\n",
      "loss: 3.250816  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 20.6%, Avg loss: 3.253577 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 3.252854  [    0/52000]\n",
      "loss: 3.252095  [ 3200/52000]\n",
      "loss: 3.256972  [ 6400/52000]\n",
      "loss: 3.253525  [ 9600/52000]\n",
      "loss: 3.252763  [12800/52000]\n",
      "loss: 3.253128  [16000/52000]\n",
      "loss: 3.252559  [19200/52000]\n",
      "loss: 3.254447  [22400/52000]\n",
      "loss: 3.252903  [25600/52000]\n",
      "loss: 3.253130  [28800/52000]\n",
      "loss: 3.252011  [32000/52000]\n",
      "loss: 3.254568  [35200/52000]\n",
      "loss: 3.253256  [38400/52000]\n",
      "loss: 3.251192  [41600/52000]\n",
      "loss: 3.252973  [44800/52000]\n",
      "loss: 3.251610  [48000/52000]\n",
      "loss: 3.250086  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 19.6%, Avg loss: 3.253244 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 3.252511  [    0/52000]\n",
      "loss: 3.251561  [ 3200/52000]\n",
      "loss: 3.256882  [ 6400/52000]\n",
      "loss: 3.253219  [ 9600/52000]\n",
      "loss: 3.252322  [12800/52000]\n",
      "loss: 3.252736  [16000/52000]\n",
      "loss: 3.251990  [19200/52000]\n",
      "loss: 3.254167  [22400/52000]\n",
      "loss: 3.252602  [25600/52000]\n",
      "loss: 3.252795  [28800/52000]\n",
      "loss: 3.251652  [32000/52000]\n",
      "loss: 3.254344  [35200/52000]\n",
      "loss: 3.252944  [38400/52000]\n",
      "loss: 3.250621  [41600/52000]\n",
      "loss: 3.252604  [44800/52000]\n",
      "loss: 3.251038  [48000/52000]\n",
      "loss: 3.249236  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 18.6%, Avg loss: 3.252877 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 3.252139  [    0/52000]\n",
      "loss: 3.250949  [ 3200/52000]\n",
      "loss: 3.256786  [ 6400/52000]\n",
      "loss: 3.252882  [ 9600/52000]\n",
      "loss: 3.251829  [12800/52000]\n",
      "loss: 3.252295  [16000/52000]\n",
      "loss: 3.251315  [19200/52000]\n",
      "loss: 3.253849  [22400/52000]\n",
      "loss: 3.252278  [25600/52000]\n",
      "loss: 3.252436  [28800/52000]\n",
      "loss: 3.251275  [32000/52000]\n",
      "loss: 3.254100  [35200/52000]\n",
      "loss: 3.252614  [38400/52000]\n",
      "loss: 3.249978  [41600/52000]\n",
      "loss: 3.252200  [44800/52000]\n",
      "loss: 3.250394  [48000/52000]\n",
      "loss: 3.248224  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 17.2%, Avg loss: 3.252469 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 3.251735  [    0/52000]\n",
      "loss: 3.250234  [ 3200/52000]\n",
      "loss: 3.256683  [ 6400/52000]\n",
      "loss: 3.252511  [ 9600/52000]\n",
      "loss: 3.251271  [12800/52000]\n",
      "loss: 3.251797  [16000/52000]\n",
      "loss: 3.250497  [19200/52000]\n",
      "loss: 3.253485  [22400/52000]\n",
      "loss: 3.251935  [25600/52000]\n",
      "loss: 3.252050  [28800/52000]\n",
      "loss: 3.250877  [32000/52000]\n",
      "loss: 3.253829  [35200/52000]\n",
      "loss: 3.252264  [38400/52000]\n",
      "loss: 3.249239  [41600/52000]\n",
      "loss: 3.251749  [44800/52000]\n",
      "loss: 3.249655  [48000/52000]\n",
      "loss: 3.246993  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 15.7%, Avg loss: 3.252009 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 3.251296  [    0/52000]\n",
      "loss: 3.249379  [ 3200/52000]\n",
      "loss: 3.256573  [ 6400/52000]\n",
      "loss: 3.252094  [ 9600/52000]\n",
      "loss: 3.250627  [12800/52000]\n",
      "loss: 3.251220  [16000/52000]\n",
      "loss: 3.249471  [19200/52000]\n",
      "loss: 3.253062  [22400/52000]\n",
      "loss: 3.251572  [25600/52000]\n",
      "loss: 3.251637  [28800/52000]\n",
      "loss: 3.250466  [32000/52000]\n",
      "loss: 3.253528  [35200/52000]\n",
      "loss: 3.251897  [38400/52000]\n",
      "loss: 3.248371  [41600/52000]\n",
      "loss: 3.251230  [44800/52000]\n",
      "loss: 3.248797  [48000/52000]\n",
      "loss: 3.245447  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 13.8%, Avg loss: 3.251481 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 3.250811  [    0/52000]\n",
      "loss: 3.248330  [ 3200/52000]\n",
      "loss: 3.256457  [ 6400/52000]\n",
      "loss: 3.251623  [ 9600/52000]\n",
      "loss: 3.249865  [12800/52000]\n",
      "loss: 3.250536  [16000/52000]\n",
      "loss: 3.248135  [19200/52000]\n",
      "loss: 3.252554  [22400/52000]\n",
      "loss: 3.251195  [25600/52000]\n",
      "loss: 3.251194  [28800/52000]\n",
      "loss: 3.250051  [32000/52000]\n",
      "loss: 3.253188  [35200/52000]\n",
      "loss: 3.251521  [38400/52000]\n",
      "loss: 3.247324  [41600/52000]\n",
      "loss: 3.250624  [44800/52000]\n",
      "loss: 3.247775  [48000/52000]\n",
      "loss: 3.243418  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 11.8%, Avg loss: 3.250859 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 3.250272  [    0/52000]\n",
      "loss: 3.246983  [ 3200/52000]\n",
      "loss: 3.256336  [ 6400/52000]\n",
      "loss: 3.251077  [ 9600/52000]\n",
      "loss: 3.248928  [12800/52000]\n",
      "loss: 3.249693  [16000/52000]\n",
      "loss: 3.246303  [19200/52000]\n",
      "loss: 3.251919  [22400/52000]\n",
      "loss: 3.250818  [25600/52000]\n",
      "loss: 3.250727  [28800/52000]\n",
      "loss: 3.249655  [32000/52000]\n",
      "loss: 3.252793  [35200/52000]\n",
      "loss: 3.251150  [38400/52000]\n",
      "loss: 3.246012  [41600/52000]\n",
      "loss: 3.249883  [44800/52000]\n",
      "loss: 3.246506  [48000/52000]\n",
      "loss: 3.240594  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 9.9%, Avg loss: 3.250096 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 3.249666  [    0/52000]\n",
      "loss: 3.245159  [ 3200/52000]\n",
      "loss: 3.256221  [ 6400/52000]\n",
      "loss: 3.250423  [ 9600/52000]\n",
      "loss: 3.247722  [12800/52000]\n",
      "loss: 3.248599  [16000/52000]\n",
      "loss: 3.243613  [19200/52000]\n",
      "loss: 3.251070  [22400/52000]\n",
      "loss: 3.250471  [25600/52000]\n",
      "loss: 3.250241  [28800/52000]\n",
      "loss: 3.249331  [32000/52000]\n",
      "loss: 3.252318  [35200/52000]\n",
      "loss: 3.250820  [38400/52000]\n",
      "loss: 3.244272  [41600/52000]\n",
      "loss: 3.248914  [44800/52000]\n",
      "loss: 3.244844  [48000/52000]\n",
      "loss: 3.236334  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 7.7%, Avg loss: 3.249102 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 3.248978  [    0/52000]\n",
      "loss: 3.242481  [ 3200/52000]\n",
      "loss: 3.256133  [ 6400/52000]\n",
      "loss: 3.249597  [ 9600/52000]\n",
      "loss: 3.246044  [12800/52000]\n",
      "loss: 3.247053  [16000/52000]\n",
      "loss: 3.239296  [19200/52000]\n",
      "loss: 3.249835  [22400/52000]\n",
      "loss: 3.250228  [25600/52000]\n",
      "loss: 3.249769  [28800/52000]\n",
      "loss: 3.249206  [32000/52000]\n",
      "loss: 3.251706  [35200/52000]\n",
      "loss: 3.250615  [38400/52000]\n",
      "loss: 3.241733  [41600/52000]\n",
      "loss: 3.247526  [44800/52000]\n",
      "loss: 3.242452  [48000/52000]\n",
      "loss: 3.229241  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 5.8%, Avg loss: 3.247683 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 3.248166  [    0/52000]\n",
      "loss: 3.238105  [ 3200/52000]\n",
      "loss: 3.256119  [ 6400/52000]\n",
      "loss: 3.248449  [ 9600/52000]\n",
      "loss: 3.243458  [12800/52000]\n",
      "loss: 3.244602  [16000/52000]\n",
      "loss: 3.231616  [19200/52000]\n",
      "loss: 3.247792  [22400/52000]\n",
      "loss: 3.250297  [25600/52000]\n",
      "loss: 3.249407  [28800/52000]\n",
      "loss: 3.249626  [32000/52000]\n",
      "loss: 3.250810  [35200/52000]\n",
      "loss: 3.250777  [38400/52000]\n",
      "loss: 3.237606  [41600/52000]\n",
      "loss: 3.245270  [44800/52000]\n",
      "loss: 3.238561  [48000/52000]\n",
      "loss: 3.216437  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 4.2%, Avg loss: 3.245438 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 3.247172  [    0/52000]\n",
      "loss: 3.230143  [ 3200/52000]\n",
      "loss: 3.256322  [ 6400/52000]\n",
      "loss: 3.246637  [ 9600/52000]\n",
      "loss: 3.239212  [12800/52000]\n",
      "loss: 3.240193  [16000/52000]\n",
      "loss: 3.217759  [19200/52000]\n",
      "loss: 3.244011  [22400/52000]\n",
      "loss: 3.251195  [25600/52000]\n",
      "loss: 3.249432  [28800/52000]\n",
      "loss: 3.251451  [32000/52000]\n",
      "loss: 3.249344  [35200/52000]\n",
      "loss: 3.251816  [38400/52000]\n",
      "loss: 3.230809  [41600/52000]\n",
      "loss: 3.241799  [44800/52000]\n",
      "loss: 3.232180  [48000/52000]\n",
      "loss: 3.196639  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 3.6%, Avg loss: 3.242179 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 3.245959  [    0/52000]\n",
      "loss: 3.217270  [ 3200/52000]\n",
      "loss: 3.256948  [ 6400/52000]\n",
      "loss: 3.243878  [ 9600/52000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5321/964668210.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearning_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_lin3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_5321/3991844215.py\u001b[0m in \u001b[0;36mlearning_cycle\u001b[0;34m(epochs, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m#print(\"Done!\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5321/3119824423.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    134\u001b[0m                         \u001b[0mmomentum_buffer_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'momentum_buffer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             F.sgd(params_with_grad,\n\u001b[0m\u001b[1;32m    137\u001b[0m                   \u001b[0md_p_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                   \u001b[0mmomentum_buffer_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov)\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_cycle(200, model_lin3, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1  \n",
    "  \n",
    "*Вес в общей оценке - 0.35*  \n",
    "  \n",
    "1. Постройте и обучите модели с 2-мя и 3-мя полносвязными (dense) скрытыми слоями.  \n",
    "При моделировании необходимо попробовать разные параметры нейронной сети - число нейронов на каждом слое, learning rate, batch size, функции активации, регуляризации и т.д. Оцените качество моделей с различными параметрами, проведите сравнительный анализ. \n",
    "2. Для наилучшей модели постройте confusion matrix результатов предсказаний модели на тестовых данных.  \n",
    "Насколько равномерно обучилась ваша модель? Приведите буквы с самой лучшей и с самой худшей точностью детекции.\n",
    "3. Найдите 10 пар букв, которые чаще всего путаются между собой, дайте возможное объяснение. Приведите примеры с картинками, которые были детектированы с ошибкой.\n",
    "4. Возьмите первую букву вашей фамилии и укажите её точность детекции. С какими буквами ваша модель чаще всего путает эту букву?     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2  \n",
    "  \n",
    "*Вес в общей оценке - 0.35*  \n",
    "  \n",
    "1. Постройте и обучите модели нейронной сети с 1-м, 2-мя и 3-мя сверточными слоями.  \n",
    "Попробуйте различные значения параметров сверток и числа фильтров на каждом слое. Оцените качество моделей с различными параметрами, проведите сравнительный анализ.  \n",
    "2. Для наилучшей конфигурации из предыдущего пункта, сравните, как меняется качество модели при увеличении размера батча при использовании BatchNorm и GroupNorm.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3    \n",
    "  \n",
    "Обучите модель с точностью (accuracy) на тестовых данных:  \n",
    "- `>= 0.85`    +1 балл\n",
    "- `>= 0.95`    +2 балла\n",
    "- `>= 0.99`    +3 балла  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бонусные задания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 1 (1 балл).**  \n",
    "\n",
    "Напишите на листке белой бумаги (маркером или ручкой) от 5 разных букв (можно больше 5 букв в целом с повторениями, но должно быть минимум 5 разных) английского алфавита (в датасете есть как прописные, так и строчные буквы). Сфотографируйте букву и приведите её картинку к размеру $28\\times28$ и, желательно, к чёрно-белой палитре цветов. Передайте получившиеся изображения вашей модели и выполните предсказание, оцените результат.  \n",
    "  \n",
    "**Tips:**  \n",
    "- В датасете все буквы занимают практически всё пространство картинки по высоте или ширине (или вместе). Если ваша буква будет слишком маленькой или большой, это может повлиять на результат детекции.\n",
    "- Помните, что буква должна быть белого цвета, а фон - чёрного.\n",
    "- Описание ваших действий при выполнении этого задания (что вы использовали, чтобы привести картинку к нужному виду) категорически приветствуется :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 2 (1 балл):**    \n",
    "  \n",
    "Используйте transfer learning подход для решения задачи - дообучите какую-либо модель, предобученную на ImageNet, для классификации рукописных букв. Оцените качество решения.  \n",
    "В качестве предобученой модели можно взять одну из [torchvision models](https://pytorch.org/vision/stable/models.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3 (1 балл):**  \n",
    "  \n",
    "Добавьте вывод значений функции потерь и accuracy в tensorboard.  \n",
    "Метрики нужно выводить и для обучающей, и для тестовой выборки."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
