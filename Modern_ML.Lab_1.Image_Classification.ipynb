{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Майнор \"Интеллектуальный анализ данных\" </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Курс \"Современные методы машинного обучения\" </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Лабораторная работа №1. Image Classification. </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной лабораторной работе вам предлагается обучить модель на основе нейронной сети для распознавания рукописных букв английского алфавита."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные представлены двумя датасетами: обучающим (`train`) и тестовым (`test`). Изображения для каждого датасета находятся в `images.zip`.  \n",
    "  \n",
    "Обучающая выборка состоит из 65000 изображений - по 2500 изображений для каждой буквы.  \n",
    "Тестовая выборка состоит из 13000 изображений - по 500 изображений для каждой буквы.  \n",
    "  \n",
    "Все изображения - монохромные (но в формате RGB), размерности $28 \\times 28$ пикселей, в формате JPEG. \n",
    "В названии каждого файла содержатся буква, которая представлена на изображении, и уникальный номер изображения: `a_00002.jpg`.  \n",
    "  \n",
    "**NB:** Все изображения представлены в перевернутом виде, для корректного отображения их нужно сначала транспонировать. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 3)\n"
     ]
    }
   ],
   "source": [
    "pic = plt.imread('images/train/a/a_00002.jpg')\n",
    "print(pic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic = np.transpose(pic, axes=(1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(pic)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ваша задача - создать и обучить модель на основе нейронной сети, которая будет предсказывать букву на картинке.  \n",
    "Обучение необходимо проводить на данных из `train`, качество модели проверять на данных из `test`.  \n",
    "Целевая метрика - accuracy.  \n",
    "Для моделирования необходимо использовать `pytorch`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основные задания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_and_targets_lst(path):\n",
    "    data=[]\n",
    "    targets=[]\n",
    "    for root, currentDirectory, files in os.walk(path):\n",
    "        for file in files:\n",
    "            pic = plt.imread(os.path.join(root, file))\n",
    "            pic = torch.tensor(np.transpose(pic, axes=(1, 0, 2)))\n",
    "            pic = torch.div(pic, 255)\n",
    "            target = ord(file[0]) - ord('a')\n",
    "            data.append(pic)\n",
    "            targets.append(target)\n",
    "    return data,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset): \n",
    "    \n",
    "    def __init__(self, x, y):\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #return {'sample': self.x[idx], 'target': self.y[idx]}\n",
    "        return self.x[idx],self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = get_data_and_targets_lst(\"images/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[60000].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[60000])\n",
    "print(y[60000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(y)):\n",
    "#     ls = [0 for a in range(26)]\n",
    "#     ls[y[i]] = 1\n",
    "#     y[i]= ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13000, 13000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(X_train, y_train)\n",
    "test_dataset = MyDataset(X_test, y_test)\n",
    "#train_dataset.__getitem__(40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([ 4,  7, 16, 15,  0, 13, 20, 25, 13, 15, 17,  1,  0,  5,  6, 19, 24, 14,\n",
      "        15,  6, 11, 20,  0,  4, 13, 22,  0, 15, 18,  7,  7,  0, 24, 14,  0, 18,\n",
      "        13, 23,  7, 18, 19, 19, 12, 17, 16,  0,  9,  7,  4, 18,  9,  7, 18, 22,\n",
      "        14,  4, 24, 19,  8, 17,  6, 18, 19,  6])\n",
      "Shape of X [N, C, H, W]:  torch.Size([64, 28, 28, 3])\n",
      "Shape of y:  torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "for batch, (X, y) in enumerate(train_dataloader):\n",
    "    print(batch)\n",
    "    print(y)\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "    #NHWC now\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            #print(pred.shape)\n",
    "            #print(loss_fn(pred, y).item())\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_cycle(epochs, model, loss_fn, optimizer):\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(train_dataloader, model, loss_fn, optimizer)\n",
    "        test(test_dataloader, model, loss_fn)\n",
    "        #print(\"Done!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28*3, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 26),\n",
    "            #nn.Softmax(dim=-1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lin3 = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_lin3.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.258257  [    0/52000]\n",
      "loss: 3.258403  [ 6400/52000]\n",
      "loss: 3.257913  [12800/52000]\n",
      "loss: 3.258225  [19200/52000]\n",
      "loss: 3.258302  [25600/52000]\n",
      "loss: 3.257796  [32000/52000]\n",
      "loss: 3.258325  [38400/52000]\n",
      "loss: 3.258126  [44800/52000]\n",
      "loss: 3.257941  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 4.7%, Avg loss: 3.258083 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.258201  [    0/52000]\n",
      "loss: 3.258363  [ 6400/52000]\n",
      "loss: 3.257861  [12800/52000]\n",
      "loss: 3.258172  [19200/52000]\n",
      "loss: 3.258260  [25600/52000]\n",
      "loss: 3.257751  [32000/52000]\n",
      "loss: 3.258265  [38400/52000]\n",
      "loss: 3.258080  [44800/52000]\n",
      "loss: 3.257884  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 4.9%, Avg loss: 3.258032 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.258146  [    0/52000]\n",
      "loss: 3.258323  [ 6400/52000]\n",
      "loss: 3.257809  [12800/52000]\n",
      "loss: 3.258119  [19200/52000]\n",
      "loss: 3.258217  [25600/52000]\n",
      "loss: 3.257705  [32000/52000]\n",
      "loss: 3.258205  [38400/52000]\n",
      "loss: 3.258033  [44800/52000]\n",
      "loss: 3.257825  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 5.1%, Avg loss: 3.257979 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.258090  [    0/52000]\n",
      "loss: 3.258283  [ 6400/52000]\n",
      "loss: 3.257757  [12800/52000]\n",
      "loss: 3.258067  [19200/52000]\n",
      "loss: 3.258173  [25600/52000]\n",
      "loss: 3.257659  [32000/52000]\n",
      "loss: 3.258143  [38400/52000]\n",
      "loss: 3.257986  [44800/52000]\n",
      "loss: 3.257767  [51200/52000]\n",
      "Test Error: \n",
      " Accuracy: 5.2%, Avg loss: 3.257927 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.258034  [    0/52000]\n",
      "loss: 3.258243  [ 6400/52000]\n",
      "loss: 3.257704  [12800/52000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6563/477399789.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearning_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_lin3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_6563/3991844215.py\u001b[0m in \u001b[0;36mlearning_cycle\u001b[0;34m(epochs, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m#print(\"Done!\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6563/3119824423.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    215\u001b[0m                             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_cycle(50, model_lin3, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model_lin3(X_train[0].reshape([1,28,28,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0387, 0.0402, 0.0384, 0.0392, 0.0462, 0.0359, 0.0386, 0.0393, 0.0349,\n",
       "         0.0335, 0.0386, 0.0407, 0.0366, 0.0372, 0.0413, 0.0396, 0.0381, 0.0401,\n",
       "         0.0369, 0.0388, 0.0400, 0.0379, 0.0381, 0.0364, 0.0374, 0.0375]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1  \n",
    "  \n",
    "*Вес в общей оценке - 0.35*  \n",
    "  \n",
    "1. Постройте и обучите модели с 2-мя и 3-мя полносвязными (dense) скрытыми слоями.  \n",
    "При моделировании необходимо попробовать разные параметры нейронной сети - число нейронов на каждом слое, learning rate, batch size, функции активации, регуляризации и т.д. Оцените качество моделей с различными параметрами, проведите сравнительный анализ. \n",
    "2. Для наилучшей модели постройте confusion matrix результатов предсказаний модели на тестовых данных.  \n",
    "Насколько равномерно обучилась ваша модель? Приведите буквы с самой лучшей и с самой худшей точностью детекции.\n",
    "3. Найдите 10 пар букв, которые чаще всего путаются между собой, дайте возможное объяснение. Приведите примеры с картинками, которые были детектированы с ошибкой.\n",
    "4. Возьмите первую букву вашей фамилии и укажите её точность детекции. С какими буквами ваша модель чаще всего путает эту букву?     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2  \n",
    "  \n",
    "*Вес в общей оценке - 0.35*  \n",
    "  \n",
    "1. Постройте и обучите модели нейронной сети с 1-м, 2-мя и 3-мя сверточными слоями.  \n",
    "Попробуйте различные значения параметров сверток и числа фильтров на каждом слое. Оцените качество моделей с различными параметрами, проведите сравнительный анализ.  \n",
    "2. Для наилучшей конфигурации из предыдущего пункта, сравните, как меняется качество модели при увеличении размера батча при использовании BatchNorm и GroupNorm.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3    \n",
    "  \n",
    "Обучите модель с точностью (accuracy) на тестовых данных:  \n",
    "- `>= 0.85`    +1 балл\n",
    "- `>= 0.95`    +2 балла\n",
    "- `>= 0.99`    +3 балла  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бонусные задания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 1 (1 балл).**  \n",
    "\n",
    "Напишите на листке белой бумаги (маркером или ручкой) от 5 разных букв (можно больше 5 букв в целом с повторениями, но должно быть минимум 5 разных) английского алфавита (в датасете есть как прописные, так и строчные буквы). Сфотографируйте букву и приведите её картинку к размеру $28\\times28$ и, желательно, к чёрно-белой палитре цветов. Передайте получившиеся изображения вашей модели и выполните предсказание, оцените результат.  \n",
    "  \n",
    "**Tips:**  \n",
    "- В датасете все буквы занимают практически всё пространство картинки по высоте или ширине (или вместе). Если ваша буква будет слишком маленькой или большой, это может повлиять на результат детекции.\n",
    "- Помните, что буква должна быть белого цвета, а фон - чёрного.\n",
    "- Описание ваших действий при выполнении этого задания (что вы использовали, чтобы привести картинку к нужному виду) категорически приветствуется :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 2 (1 балл):**    \n",
    "  \n",
    "Используйте transfer learning подход для решения задачи - дообучите какую-либо модель, предобученную на ImageNet, для классификации рукописных букв. Оцените качество решения.  \n",
    "В качестве предобученой модели можно взять одну из [torchvision models](https://pytorch.org/vision/stable/models.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3 (1 балл):**  \n",
    "  \n",
    "Добавьте вывод значений функции потерь и accuracy в tensorboard.  \n",
    "Метрики нужно выводить и для обучающей, и для тестовой выборки."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
